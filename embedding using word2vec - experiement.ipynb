{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 (failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-9e59ea4d57c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     wvec = {line.split()[0].decode(encoding): np.array(line.split()[1:],dtype=np.float32)\n\u001b[0;32m      4\u001b[0m                for line in lines}\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not Series"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "with open(rose['text'], \"rb\") as lines:\n",
    "    wvec = {line.split()[0].decode(encoding): np.array(line.split()[1:],dtype=np.float32)\n",
    "               for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rose = pd.read_excel('rose.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = rose['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 132)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "text_counts = count_vect.fit_transform(text)\n",
    "text_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('e', array([-0.02027582, -0.01239222,  0.01349756,  0.04614235, -0.03881368,\n",
       "       -0.01403142, -0.05428512,  0.03362289,  0.0018558 , -0.00130165],\n",
       "      dtype=float32)), ('n', array([-0.03825945, -0.02811386, -0.04106112,  0.03083437,  0.01579047,\n",
       "       -0.01286134, -0.01877186, -0.01159137, -0.00805879,  0.04646649],\n",
       "      dtype=float32)), ('t', array([ 0.02675523,  0.00614828,  0.0369582 ,  0.00727019,  0.00831095,\n",
       "        0.04119881, -0.04494306, -0.02176107,  0.00568487, -0.0135679 ],\n",
       "      dtype=float32)), ('s', array([-0.00458   , -0.00543844, -0.03407583, -0.00620107,  0.04362049,\n",
       "        0.0066507 , -0.03984213,  0.0352293 ,  0.00365619,  0.00470684],\n",
       "      dtype=float32)), ('o', array([ 0.00030056, -0.02137967,  0.00631244,  0.0297047 , -0.03652196,\n",
       "        0.00408429, -0.03630996, -0.04031385, -0.03121908, -0.03324112],\n",
       "      dtype=float32)), ('i', array([ 0.04612219, -0.04068083,  0.00113151, -0.03134857,  0.0058077 ,\n",
       "        0.04784923, -0.01483676,  0.04796913,  0.04902454, -0.01588334],\n",
       "      dtype=float32)), ('r', array([-0.02425317, -0.00980502,  0.00499979, -0.01327446, -0.00061937,\n",
       "        0.03184584,  0.03839022, -0.02151045,  0.02864781,  0.03422608],\n",
       "      dtype=float32)), ('a', array([ 0.03450095,  0.02851288, -0.03844091, -0.023316  ,  0.01060325,\n",
       "       -0.00505674,  0.00775329, -0.03206115,  0.00026635,  0.04502427],\n",
       "      dtype=float32)), ('l', array([-0.01378035,  0.00521494,  0.04539771, -0.00077311, -0.01498809,\n",
       "       -0.01853087,  0.0283118 ,  0.02333521, -0.04689109,  0.0496305 ],\n",
       "      dtype=float32)), ('c', array([ 0.00218672,  0.01433075, -0.01850558, -0.02735852,  0.01520267,\n",
       "        0.01370235, -0.05078376,  0.01595403,  0.04186562, -0.04059516],\n",
       "      dtype=float32)), ('u', array([ 0.02823438, -0.0411929 ,  0.04365268, -0.0372595 ,  0.04590585,\n",
       "       -0.01577566, -0.03227176,  0.02748031,  0.01611399,  0.0376594 ],\n",
       "      dtype=float32)), ('m', array([-0.00474679, -0.0444224 ,  0.01177401,  0.01910121, -0.03518465,\n",
       "        0.04406446, -0.03842521,  0.02724104,  0.0373062 , -0.00284517],\n",
       "      dtype=float32)), ('d', array([-0.03916686, -0.03192453, -0.02627061,  0.01099729, -0.02392872,\n",
       "        0.03926137, -0.02760738, -0.04212639,  0.00456795,  0.04611384],\n",
       "      dtype=float32)), ('p', array([-0.02969007,  0.03473537,  0.04890542, -0.04377726, -0.03339179,\n",
       "        0.0385587 ,  0.01823362, -0.04279169, -0.03547578, -0.03557637],\n",
       "      dtype=float32)), ('h', array([-0.04090247,  0.02642878,  0.019995  ,  0.03390312,  0.0100152 ,\n",
       "        0.04464495, -0.00523219,  0.03811451, -0.01540411,  0.01200682],\n",
       "      dtype=float32)), ('g', array([ 0.04942008, -0.04784902, -0.00628302, -0.01436806,  0.04711361,\n",
       "        0.00769022, -0.01433766,  0.00529602,  0.0147467 ,  0.03305571],\n",
       "      dtype=float32)), ('b', array([ 0.0435975 , -0.02875984, -0.04700137, -0.04331003, -0.03347551,\n",
       "        0.02808549, -0.00859318,  0.00550072,  0.00275   ,  0.00330129],\n",
       "      dtype=float32)), ('y', array([-0.01786871, -0.0385942 , -0.03892007, -0.03976469,  0.00536904,\n",
       "       -0.04554299, -0.04667882, -0.00510006, -0.00718595, -0.02706492],\n",
       "      dtype=float32)), ('w', array([ 0.04558513,  0.02997386,  0.03699142,  0.04478739, -0.00102593,\n",
       "        0.00092347, -0.0063553 , -0.03944894,  0.0180684 , -0.04930233],\n",
       "      dtype=float32)), ('f', array([ 0.04856236,  0.01109353, -0.00954378,  0.02319452, -0.00259629,\n",
       "       -0.01335426, -0.02913061,  0.0225487 ,  0.01072073,  0.01534488],\n",
       "      dtype=float32)), ('k', array([ 0.00353342, -0.00778196,  0.03156613, -0.04951629,  0.0357577 ,\n",
       "       -0.00061942, -0.03337384,  0.03605387,  0.01255544, -0.04511094],\n",
       "      dtype=float32)), ('v', array([ 0.00338707, -0.04549532,  0.01047867, -0.03595059,  0.04604767,\n",
       "        0.01221441, -0.02362169,  0.02682057,  0.00247964, -0.04553945],\n",
       "      dtype=float32)), ('x', array([-0.00441804, -0.02074064, -0.01313103, -0.01981147, -0.02107288,\n",
       "       -0.01867922,  0.03938077, -0.02131667,  0.01814282,  0.01370626],\n",
       "      dtype=float32))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "model = Word2Vec(count_vect.get_feature_names(), size=100, window=5, min_count=5, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_small' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-39eaecb62397>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMeanEmbeddingVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-23915c2bbfec>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, word2vec)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_small\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_small' is not defined"
     ]
    }
   ],
   "source": [
    "m = MeanEmbeddingVectorizer(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'itervalues'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-0a8fd82acaac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m etree_w2v = Pipeline([\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[1;34m\"word2vec vectorizer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMeanEmbeddingVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
      "\u001b[1;32m<ipython-input-12-073d10bd9224>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, word2vec)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# if a text is empty we should return a vector of zeros\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# with the same dimensionality as all the other vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'itervalues'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 \n",
    "http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XO7n94hKg2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is a pink rose</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the pink rose grows in the mud</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the pink rose is not black</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everybody like the pink rose</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the blue rose is not black</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blue roses are rare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Unlimited talk and text to US and Canada. Our ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Text messaging, or texting, is the act of comp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Textfree is the free texting and free calling ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Send &amp; receive text messages in Messages. You ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Text definition is - the original words and fo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Text Request is an online text messaging servi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Text Request is an online text messaging servi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Unlimited texts and calls to the US &amp; Canada. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Unlike the .html() method, .text() can be used...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  target\n",
       "0                                 this is a pink rose       0\n",
       "1                      the pink rose grows in the mud       0\n",
       "2                          the pink rose is not black       1\n",
       "3                        everybody like the pink rose       1\n",
       "4                          the blue rose is not black       1\n",
       "5                                 blue roses are rare       1\n",
       "6   Unlimited talk and text to US and Canada. Our ...       0\n",
       "7   Text messaging, or texting, is the act of comp...       1\n",
       "8   Textfree is the free texting and free calling ...       0\n",
       "9   Send & receive text messages in Messages. You ...       2\n",
       "10  Text definition is - the original words and fo...       2\n",
       "11  Text Request is an online text messaging servi...       2\n",
       "12  Text Request is an online text messaging servi...       1\n",
       "13  Unlimited texts and calls to the US & Canada. ...       0\n",
       "14  Unlike the .html() method, .text() can be used...       0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "rose['text'].to_csv('rosetext.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'this is a pink rose\\r\\n'\n",
      "b'the pink rose grows in the mud\\r\\n'\n",
      "b'the pink rose is not black\\r\\n'\n",
      "b'everybody like the pink rose\\r\\n'\n",
      "b'the blue rose is not black\\r\\n'\n",
      "b'blue roses are rare\\r\\n'\n",
      "b\"Unlimited talk and\\xc2\\xa0text\\xc2\\xa0to US and Canada. Our phones come with a 30-day money-back guarantee and 1 year warranty. If you aren't satisfied just send your\\xc2\\xa0.\\r\\n\"\n",
      "b'\"Text\\xc2\\xa0messaging, or\\xc2\\xa0texting, is the act of composing and sending electronic messages, typically consisting of alphabetic and numeric characters, between two or more users of mobile devices, desktops/laptops, or other type of compatible computer.\"\\r\\n'\n",
      "b'\"Textfree is the free texting and free calling app that gives you free\\xc2\\xa0text\\xc2\\xa0plus a real US phone number so you can\\xc2\\xa0text\\xc2\\xa0anyone, even if they don\\'t have the app.\"\\r\\n'\n",
      "b'Send & receive\\xc2\\xa0text\\xc2\\xa0messages in Messages. You can send and receive\\xc2\\xa0text\\xc2\\xa0messages with friends and contacts on Messages.\\r\\n'\n",
      "b'Text\\xc2\\xa0definition is - the original words and form of a written or printed work. How to use\\xc2\\xa0text\\xc2\\xa0in a sentence.\\r\\n'\n",
      "b'Text\\xc2\\xa0Request is an online\\xc2\\xa0text\\xc2\\xa0messaging service for small businesses. Login from any device to send and receive\\xc2\\xa0text\\xc2\\xa0messages through your current business\\xc2\\xa0\\r\\n'\n",
      "b'Text\\xc2\\xa0Request is an online\\xc2\\xa0text\\xc2\\xa0messaging service for small businesses. Login from any device to send and receive\\xc2\\xa0text\\xc2\\xa0messages through your current business\\xc2\\xa0\\r\\n'\n",
      "b'\"Unlimited\\xc2\\xa0texts\\xc2\\xa0and calls to the US & Canada. Your own real phone number! The best free texting app on the store with free calling and free, multiple phon\"\\r\\n'\n",
      "b'\"Unlike the .html() method, .text() can be used in both XML and HTML documents. The result of the .text() method is a string containing the combined\\xc2\\xa0text\\xc2\\xa0of all\"\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "with open('rosetext.csv', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with open(input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input ('rosetext.csv'))\n",
    "logging.info (\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'pink', 'rose'],\n",
       " ['the', 'pink', 'rose', 'grows', 'in', 'the', 'mud'],\n",
       " ['the', 'pink', 'rose', 'is', 'not', 'black'],\n",
       " ['everybody', 'like', 'the', 'pink', 'rose'],\n",
       " ['the', 'blue', 'rose', 'is', 'not', 'black'],\n",
       " ['blue', 'roses', 'are', 'rare'],\n",
       " ['unlimited',\n",
       "  'talk',\n",
       "  'and',\n",
       "  'text',\n",
       "  'to',\n",
       "  'us',\n",
       "  'and',\n",
       "  'canada',\n",
       "  'our',\n",
       "  'phones',\n",
       "  'come',\n",
       "  'with',\n",
       "  'day',\n",
       "  'money',\n",
       "  'back',\n",
       "  'guarantee',\n",
       "  'and',\n",
       "  'year',\n",
       "  'warranty',\n",
       "  'if',\n",
       "  'you',\n",
       "  'aren',\n",
       "  'satisfied',\n",
       "  'just',\n",
       "  'send',\n",
       "  'your'],\n",
       " ['text',\n",
       "  'messaging',\n",
       "  'or',\n",
       "  'texting',\n",
       "  'is',\n",
       "  'the',\n",
       "  'act',\n",
       "  'of',\n",
       "  'composing',\n",
       "  'and',\n",
       "  'sending',\n",
       "  'electronic',\n",
       "  'messages',\n",
       "  'typically',\n",
       "  'consisting',\n",
       "  'of',\n",
       "  'alphabetic',\n",
       "  'and',\n",
       "  'numeric',\n",
       "  'characters',\n",
       "  'between',\n",
       "  'two',\n",
       "  'or',\n",
       "  'more',\n",
       "  'users',\n",
       "  'of',\n",
       "  'mobile',\n",
       "  'devices',\n",
       "  'desktops',\n",
       "  'laptops',\n",
       "  'or',\n",
       "  'other',\n",
       "  'type',\n",
       "  'of',\n",
       "  'compatible',\n",
       "  'computer'],\n",
       " ['textfree',\n",
       "  'is',\n",
       "  'the',\n",
       "  'free',\n",
       "  'texting',\n",
       "  'and',\n",
       "  'free',\n",
       "  'calling',\n",
       "  'app',\n",
       "  'that',\n",
       "  'gives',\n",
       "  'you',\n",
       "  'free',\n",
       "  'text',\n",
       "  'plus',\n",
       "  'real',\n",
       "  'us',\n",
       "  'phone',\n",
       "  'number',\n",
       "  'so',\n",
       "  'you',\n",
       "  'can',\n",
       "  'text',\n",
       "  'anyone',\n",
       "  'even',\n",
       "  'if',\n",
       "  'they',\n",
       "  'don',\n",
       "  'have',\n",
       "  'the',\n",
       "  'app'],\n",
       " ['send',\n",
       "  'receive',\n",
       "  'text',\n",
       "  'messages',\n",
       "  'in',\n",
       "  'messages',\n",
       "  'you',\n",
       "  'can',\n",
       "  'send',\n",
       "  'and',\n",
       "  'receive',\n",
       "  'text',\n",
       "  'messages',\n",
       "  'with',\n",
       "  'friends',\n",
       "  'and',\n",
       "  'contacts',\n",
       "  'on',\n",
       "  'messages'],\n",
       " ['text',\n",
       "  'definition',\n",
       "  'is',\n",
       "  'the',\n",
       "  'original',\n",
       "  'words',\n",
       "  'and',\n",
       "  'form',\n",
       "  'of',\n",
       "  'written',\n",
       "  'or',\n",
       "  'printed',\n",
       "  'work',\n",
       "  'how',\n",
       "  'to',\n",
       "  'use',\n",
       "  'text',\n",
       "  'in',\n",
       "  'sentence'],\n",
       " ['text',\n",
       "  'request',\n",
       "  'is',\n",
       "  'an',\n",
       "  'online',\n",
       "  'text',\n",
       "  'messaging',\n",
       "  'service',\n",
       "  'for',\n",
       "  'small',\n",
       "  'businesses',\n",
       "  'login',\n",
       "  'from',\n",
       "  'any',\n",
       "  'device',\n",
       "  'to',\n",
       "  'send',\n",
       "  'and',\n",
       "  'receive',\n",
       "  'text',\n",
       "  'messages',\n",
       "  'through',\n",
       "  'your',\n",
       "  'current',\n",
       "  'business'],\n",
       " ['text',\n",
       "  'request',\n",
       "  'is',\n",
       "  'an',\n",
       "  'online',\n",
       "  'text',\n",
       "  'messaging',\n",
       "  'service',\n",
       "  'for',\n",
       "  'small',\n",
       "  'businesses',\n",
       "  'login',\n",
       "  'from',\n",
       "  'any',\n",
       "  'device',\n",
       "  'to',\n",
       "  'send',\n",
       "  'and',\n",
       "  'receive',\n",
       "  'text',\n",
       "  'messages',\n",
       "  'through',\n",
       "  'your',\n",
       "  'current',\n",
       "  'business'],\n",
       " ['unlimited',\n",
       "  'texts',\n",
       "  'and',\n",
       "  'calls',\n",
       "  'to',\n",
       "  'the',\n",
       "  'us',\n",
       "  'canada',\n",
       "  'your',\n",
       "  'own',\n",
       "  'real',\n",
       "  'phone',\n",
       "  'number',\n",
       "  'the',\n",
       "  'best',\n",
       "  'free',\n",
       "  'texting',\n",
       "  'app',\n",
       "  'on',\n",
       "  'the',\n",
       "  'store',\n",
       "  'with',\n",
       "  'free',\n",
       "  'calling',\n",
       "  'and',\n",
       "  'free',\n",
       "  'multiple',\n",
       "  'phon'],\n",
       " ['unlike',\n",
       "  'the',\n",
       "  'html',\n",
       "  'method',\n",
       "  'text',\n",
       "  'can',\n",
       "  'be',\n",
       "  'used',\n",
       "  'in',\n",
       "  'both',\n",
       "  'xml',\n",
       "  'and',\n",
       "  'html',\n",
       "  'documents',\n",
       "  'the',\n",
       "  'result',\n",
       "  'of',\n",
       "  'the',\n",
       "  'text',\n",
       "  'method',\n",
       "  'is',\n",
       "  'string',\n",
       "  'containing',\n",
       "  'the',\n",
       "  'combined',\n",
       "  'text',\n",
       "  'of',\n",
       "  'all']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(501, 2690)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('real', 0.17798131704330444),\n",
       " ('login', 0.1543339490890503),\n",
       " ('calling', 0.1363137662410736),\n",
       " ('us', 0.1281176060438156),\n",
       " ('of', 0.12339971959590912),\n",
       " ('send', 0.11913784593343735),\n",
       " ('through', 0.1178809329867363),\n",
       " ('canada', 0.11634750664234161),\n",
       " ('business', 0.09261417388916016),\n",
       " ('free', 0.09156455099582672)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 \n",
    "https://www.kaggle.com/arunava21/word2vec-and-random-forest-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, word2vec\n",
    "import gensim\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sqlite3\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34564, 45)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rose = pd.read_excel('rose.xlsx')\n",
    "rose = pd.read_excel('incident V2 - Enriched.xlsx')\n",
    "rose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30874, 45)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rose = rose[rose['state'].isin(['Closed', 'Closed (CR Implemented)','Closed (Purchase Required)', 'Resolved'])].copy()\n",
    "rose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30874, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rose = rose[['description', 'ag']].copy()\n",
    "rose.columns = ['text', 'target']\n",
    "rose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rose['text'] = rose['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(rose) * 0.8)\n",
    "train_reviews = rose.iloc[:train_size,:]\n",
    "test_reviews = rose.iloc[train_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24699, 2), (6175, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews.shape, test_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Convert a review to a list of words. Removal of stop words is optional.\n",
    "    \"\"\"\n",
    "    # remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "    \n",
    "    # convert to lower case and split at whitespace\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    # remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Split review into list of sentences where each sentence is a list of words.\n",
    "    Removal of stop words is optional.\n",
    "    \"\"\"\n",
    "    # use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    # each sentence is furthermore split into words\n",
    "    sentences = []    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sureshsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the punkt tokenizer used for splitting reviews into sentences\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train_sentences = []  # Initialize an empty list of sentences\n",
    "for review in train_reviews['text']:\n",
    "    train_sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(train_sentences, size=128, window=2, min_count=2, workers=4, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70108282, 89512550)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(train_sentences,total_examples=len(train_sentences),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46837, 128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all reviews\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for review in reviews:\n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return review_feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sureshsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# calculate average feature vectors for training and test sets\n",
    "num_features=128\n",
    "clean_train_reviews = []\n",
    "for review in train_reviews['text']:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "trainDataVecs = get_avg_feature_vecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clean_test_reviews = []\n",
    "for review in test_reviews['text']:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "testDataVecs = get_avg_feature_vecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24699, 128)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 78 instances from train set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# remove instances in train set that could not be represented as feature vectors\n",
    "nan_indices = list({x for x,y in np.argwhere(np.isnan(trainDataVecs))})\n",
    "if len(nan_indices) > 0:\n",
    "    print('Removing {:d} instances from train set.'.format(len(nan_indices)))\n",
    "    trainDataVecs = np.delete(trainDataVecs, nan_indices, axis=0)\n",
    "    train_reviews.drop(train_reviews.iloc[nan_indices, :].index, axis=0, inplace=True)\n",
    "    assert trainDataVecs.shape[0] == len(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 128, n_jobs=-1)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(trainDataVecs, train_reviews['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 20 instances from test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sureshsu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# remove instances in test set that could not be represented as feature vectors\n",
    "nan_indices = list({x for x,y in np.argwhere(np.isnan(testDataVecs))})\n",
    "if len(nan_indices) > 0:\n",
    "    print('Removing {:d} instances from test set.'.format(len(nan_indices)))\n",
    "    testDataVecs = np.delete(testDataVecs, nan_indices, axis=0)\n",
    "    test_reviews.drop(test_reviews.iloc[nan_indices, :].index, axis=0, inplace=True)\n",
    "    assert testDataVecs.shape[0] == len(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for test data..\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting labels for test data..\")\n",
    "result = forest.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35743298131600326"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_reviews['target'], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('help', 0.47340360283851624),\n",
       " ('consult', 0.45881205797195435),\n",
       " ('desk', 0.44597360491752625),\n",
       " ('bsas', 0.4140302538871765),\n",
       " ('dbas', 0.4030960202217102),\n",
       " ('dba', 0.40267786383628845),\n",
       " ('colleagues', 0.3985521197319031),\n",
       " ('procurement', 0.3808389902114868),\n",
       " ('billruncomm', 0.37541571259498596),\n",
       " ('esd', 0.3704761564731598),\n",
       " ('wnexmb', 0.3647332191467285),\n",
       " ('comunidade', 0.35747382044792175),\n",
       " ('servicenow', 0.3564300537109375),\n",
       " ('log', 0.3546685576438904),\n",
       " ('paysonnel', 0.3480268120765686),\n",
       " ('friends', 0.34560897946357727),\n",
       " ('itsoc', 0.34555569291114807),\n",
       " ('contact', 0.34363847970962524),\n",
       " ('iprocurement', 0.3427566885948181),\n",
       " ('assistance', 0.3412788510322571),\n",
       " ('bsa', 0.33042746782302856),\n",
       " ('hyperian', 0.3303166627883911),\n",
       " ('ticket', 0.3291030824184418),\n",
       " ('snow', 0.3228023052215576),\n",
       " ('collegues', 0.32042068243026733),\n",
       " ('mobility', 0.3191124200820923),\n",
       " ('hookup', 0.31502488255500793),\n",
       " ('sir', 0.3139888644218445),\n",
       " ('mnda', 0.31268274784088135),\n",
       " ('purchasing', 0.30988791584968567),\n",
       " ('maria', 0.3074866831302643),\n",
       " ('sirs', 0.30747467279434204),\n",
       " ('portalods', 0.30737000703811646),\n",
       " ('fs', 0.30598387122154236),\n",
       " ('mobotix', 0.3056938052177429),\n",
       " ('ask', 0.3045663833618164),\n",
       " ('proceed', 0.302118718624115),\n",
       " ('genjp', 0.30107975006103516),\n",
       " ('team', 0.30069437623023987),\n",
       " ('dpresence', 0.2996120750904083),\n",
       " ('incident', 0.2985168695449829),\n",
       " ('just', 0.29790759086608887),\n",
       " ('confer', 0.2978782057762146),\n",
       " ('avanta', 0.2978709936141968),\n",
       " ('develop', 0.2973615527153015),\n",
       " ('reference', 0.2961207330226898),\n",
       " ('wes', 0.2961134612560272),\n",
       " ('reopen', 0.29425883293151855),\n",
       " ('support', 0.2933063507080078),\n",
       " ('college', 0.29179978370666504),\n",
       " ('sounded', 0.29179105162620544),\n",
       " ('dev', 0.2883768081665039),\n",
       " ('someone', 0.288253515958786),\n",
       " ('also', 0.28677675127983093),\n",
       " ('micahel', 0.2849465012550354),\n",
       " ('holly', 0.28455808758735657),\n",
       " ('raj', 0.2837797701358795),\n",
       " ('seek', 0.28374147415161133),\n",
       " ('pause', 0.28342971205711365),\n",
       " ('urgently', 0.283363938331604),\n",
       " ('financial', 0.28319162130355835),\n",
       " ('analyse', 0.28306519985198975),\n",
       " ('crossing', 0.28072649240493774),\n",
       " ('lavanya', 0.2801809310913086),\n",
       " ('ritm', 0.28005772829055786),\n",
       " ('folks', 0.27966660261154175),\n",
       " ('desi', 0.27933791279792786),\n",
       " ('demand', 0.27886274456977844),\n",
       " ('obtain', 0.2782423198223114),\n",
       " ('kindly', 0.27798545360565186),\n",
       " ('madhav', 0.27677905559539795),\n",
       " ('gentlemen', 0.2764362394809723),\n",
       " ('assin', 0.27614226937294006),\n",
       " ('restart', 0.27598458528518677),\n",
       " ('testers', 0.2755924463272095),\n",
       " ('solution', 0.2753515839576721),\n",
       " ('assign', 0.27472686767578125),\n",
       " ('look', 0.2746700048446655),\n",
       " ('resend', 0.27412235736846924),\n",
       " ('accountname', 0.2733411490917206),\n",
       " ('crate', 0.27269959449768066),\n",
       " ('contour', 0.27236902713775635),\n",
       " ('riccardo', 0.27234357595443726),\n",
       " ('svetlana', 0.27208587527275085),\n",
       " ('read', 0.2720795273780823),\n",
       " ('raise', 0.27104490995407104),\n",
       " ('premises', 0.27057841420173645),\n",
       " ('finops', 0.2695721387863159),\n",
       " ('iexpense', 0.2690240740776062),\n",
       " ('corinne', 0.26897913217544556),\n",
       " ('request', 0.2688624858856201),\n",
       " ('fix', 0.2677939534187317),\n",
       " ('ngtvxwiff', 0.2672504782676697),\n",
       " ('requestors', 0.26674264669418335),\n",
       " ('fp', 0.2661151885986328),\n",
       " ('shanna', 0.2659614384174347),\n",
       " ('valued', 0.26593807339668274),\n",
       " ('dashboard', 0.2652127146720886),\n",
       " ('teams', 0.2651693820953369),\n",
       " ('padhu', 0.26459112763404846)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('helpdesk', topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('partnercentral', 0.5624165534973145),\n",
       " ('jpebsitsupport', 0.4998823404312134),\n",
       " ('esdecoleadsupport', 0.4862039387226105),\n",
       " ('ibxops', 0.4847724735736847),\n",
       " ('operations', 0.4780731201171875),\n",
       " ('apacordersupport', 0.4762503504753113),\n",
       " ('financial', 0.47356581687927246),\n",
       " ('legalemeasupport', 0.4623432755470276),\n",
       " ('capmgmt', 0.45611822605133057),\n",
       " ('apac', 0.44485199451446533)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['finance'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('qrs', 0.5401618480682373),\n",
       " ('country', 0.5231621861457825),\n",
       " ('trs', 0.4748820662498474),\n",
       " ('german', 0.4576883316040039),\n",
       " ('le', 0.4484444260597229),\n",
       " ('rollup', 0.43891143798828125),\n",
       " ('employees', 0.4313257336616516),\n",
       " ('acquisition', 0.43077361583709717),\n",
       " ('whose', 0.425498366355896),\n",
       " ('account', 0.419788122177124)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('legal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iprocurement', 0.47603046894073486),\n",
       " ('sit', 0.47257912158966064),\n",
       " ('production', 0.4719308018684387),\n",
       " ('billtest', 0.4475070834159851),\n",
       " ('revpro', 0.4454643726348877),\n",
       " ('sv', 0.42614835500717163),\n",
       " ('bi', 0.42557933926582336),\n",
       " ('consistency', 0.42504629492759705),\n",
       " ('clone', 0.4220660924911499),\n",
       " ('ebs', 0.4150271415710449)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('vertex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
