{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare multiple algorithms after blending text and feature based predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.ensemble import forest\n",
    "import scipy\n",
    "from scipy.cluster import hierarchy as hc\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMModel\n",
    "#Run xgboost on dataframe\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import LinearSVC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34564, 45)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data (do only once)\n",
    "cases = pd.read_excel('incident V2 - Enriched.xlsx')\n",
    "cases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def rmse(x,y): \n",
    "    return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)\n",
    "\n",
    "def split_vals(a,n): \n",
    "    return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "def get_oob(df):\n",
    "    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n",
    "    x, _ = split_vals(df, n_trn)\n",
    "    m.fit(x, y_train)\n",
    "    return m.oob_score_\n",
    "\n",
    "def add_datepart(df, fldname, drop=True, time=False):\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "        \n",
    "def train_cats(df):\n",
    "    for n,c in df.items():\n",
    "        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n",
    "\n",
    "def fix_missing(df, col, name, na_dict):\n",
    "    if is_numeric_dtype(col):\n",
    "        if pd.isnull(col).sum() or (name in na_dict):\n",
    "            df[name+'_na'] = pd.isnull(col)\n",
    "            filler = na_dict[name] if name in na_dict else col.median()\n",
    "            df[name] = col.fillna(filler)\n",
    "            na_dict[name] = filler\n",
    "    return na_dict\n",
    "\n",
    "def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n",
    "            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n",
    "    if not ignore_flds: ignore_flds=[]\n",
    "    if not skip_flds: skip_flds=[]\n",
    "    if subset: df = get_sample(df,subset)\n",
    "    else: df = df.copy()\n",
    "    ignored_flds = df.loc[:, ignore_flds]\n",
    "    df.drop(ignore_flds, axis=1, inplace=True)\n",
    "    if preproc_fn: preproc_fn(df)\n",
    "    if y_fld is None: y = None\n",
    "    else:\n",
    "        if not is_numeric_dtype(df[y_fld]): df[y_fld] = df[y_fld].cat.codes\n",
    "        y = df[y_fld].values\n",
    "        skip_flds += [y_fld]\n",
    "    df.drop(skip_flds, axis=1, inplace=True)\n",
    "\n",
    "    if na_dict is None: na_dict = {}\n",
    "    else: na_dict = na_dict.copy()\n",
    "    na_dict_initial = na_dict.copy()\n",
    "    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n",
    "    if len(na_dict_initial.keys()) > 0:\n",
    "        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n",
    "    if do_scale: mapper = scale_vars(df, mapper)\n",
    "    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n",
    "    df = pd.get_dummies(df, dummy_na=True)\n",
    "    df = pd.concat([ignored_flds, df], axis=1)\n",
    "    res = [df, y, na_dict]\n",
    "    if do_scale: res = res + [mapper]\n",
    "    return res\n",
    "\n",
    "def numericalize(df, col, name, max_n_cat):\n",
    "    if not is_numeric_dtype(col) and ( max_n_cat is None or col.nunique()>max_n_cat):\n",
    "        df[name] = col.cat.codes+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "no_of_ags = 15 #Number of AGs to consider when choosing AGs with most frequency\n",
    "#AGs to exclude from analysis.  Leave blank if none is excluded. This may change depending on years chosen\n",
    "remove_ags = ['Global Helpdesk - Tier 1', 'Japan Helpdesk Support','Global Helpdesk','Global ITSOC - Tier 1' ] #for 2018 and 2019\n",
    "#Define test and train sets cases['opened_at'].dt.to_period('M')\n",
    "test_period = ['2019-02', '2019-03', '2019-04']\n",
    "train_period = ['2018-01', '2018-02', '2018-03', '2018-04','2018-05', '2018-06', '2018-07', '2018-08','2018-09', '2018-10', '2018-11', '2018-12', '2019-01']\n",
    "#rounds = 100 #Number of times RF is run\n",
    "# Whether to merge AGs or not\n",
    "merge_ags = 'Y' #Set to 'N' if you dont want to merge AGs\n",
    "model_params = dict(((k, eval(k)) for k in ('no_of_ags', 'remove_ags', 'test_period','train_period', 'merge_ags' )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_of_ags': 15,\n",
       " 'remove_ags': ['Global Helpdesk - Tier 1',\n",
       "  'Japan Helpdesk Support',\n",
       "  'Global Helpdesk',\n",
       "  'Global ITSOC - Tier 1'],\n",
       " 'test_period': ['2019-02', '2019-03', '2019-04'],\n",
       " 'train_period': ['2018-01',\n",
       "  '2018-02',\n",
       "  '2018-03',\n",
       "  '2018-04',\n",
       "  '2018-05',\n",
       "  '2018-06',\n",
       "  '2018-07',\n",
       "  '2018-08',\n",
       "  '2018-09',\n",
       "  '2018-10',\n",
       "  '2018-11',\n",
       "  '2018-12',\n",
       "  '2019-01'],\n",
       " 'merge_ags': 'Y'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (34564, 45)\n",
      "Only closed cases shape: (30874, 45)\n",
      "Shape after selecting period (5834, 47)\n",
      "\n",
      "Shape of data subset:  (5453, 47)\n",
      "\n",
      "% of cases considered after taking subset:  93.46931779225231\n",
      "\n",
      "AG list and frequencies\n",
      "Merged Finance Support IT BSA - Finance    3182\n",
      "IT BSA - Billing C&C                       1171\n",
      "RevOps Support                              362\n",
      "Bus - Billing C&C                           203\n",
      "IT BSA - Singleview Ops                     170\n",
      "IT BSA - Vertex                             149\n",
      "Global DBA Support                           74\n",
      "Hyperion Team                                54\n",
      "IT BSA - BI Team                             52\n",
      "Singleview Admin                             23\n",
      "IT BSA - Client Services                     13\n",
      "Name: ag, dtype: int64\n",
      "\n",
      "AG to codes mapping\n",
      "{0: 'Bus - Billing C&C', 1: 'Global DBA Support', 2: 'Hyperion Team', 3: 'IT BSA - BI Team', 4: 'IT BSA - Billing C&C', 5: 'IT BSA - Client Services', 6: 'IT BSA - Singleview Ops', 7: 'IT BSA - Vertex', 8: 'Merged Finance Support IT BSA - Finance', 9: 'RevOps Support', 10: 'Singleview Admin'}\n"
     ]
    }
   ],
   "source": [
    "# Use copy so that we dont have import data for every run\n",
    "df = cases.copy()\n",
    "print('Full dataset shape:', df.shape)\n",
    "\n",
    "# Use only closed cases\n",
    "df = df[df['state'].isin(['Closed', 'Closed (CR Implemented)', 'Closed (Purchase Required)', 'Resolved'])].copy()\n",
    "print('Only closed cases shape:', df.shape)\n",
    "\n",
    "if merge_ags == 'Y':\n",
    "    df['ag_merged'] = np.where(df['ag'].isin(['Finance Support','IT BSA - Finance']), 'Merged Finance Support IT BSA - Finance', df['ag'])\n",
    "    #Reset column names for convenience\n",
    "    df.rename(columns={'ag': 'ag_old'},inplace=True)\n",
    "    df.rename(columns={'ag_merged':'ag'}, inplace=True)\\\n",
    "\n",
    "#Create new text feature\n",
    "for cols in ['short_description', 'description', 'Requester Cost Center Descr', 'Requester Location Desc']:\n",
    "    df[cols] = df[cols].astype(str)\n",
    "df['fulltext'] = df['short_description'] + ' ' + df['description'] + ' ' + df['Requester Cost Center Descr'] + ' ' + df['Requester Location Desc'] \n",
    "\n",
    "#Filter cases based on period chosen\n",
    "df['opened_at'] = pd.to_datetime(df['opened_at'])\n",
    "df = df[df['opened_at'].dt.to_period('M').astype(str).isin(test_period + train_period)].copy()\n",
    "print('Shape after selecting period', df.shape)\n",
    "\n",
    "df_size = len(df)\n",
    "\n",
    "#Filter cases based on AGs\n",
    "keep_ag = list(df['ag'].value_counts().head(no_of_ags).index)\n",
    "for i in remove_ags:\n",
    "    keep_ag.remove(i)\n",
    "df = df[df['ag'].isin(keep_ag)].copy()\n",
    "print()\n",
    "print('Shape of data subset: ', df.shape)\n",
    "\n",
    "\n",
    "#Percentage of cases considered\n",
    "print()\n",
    "print('% of cases considered after taking subset: ', len(df)*100/df_size)\n",
    "\n",
    "#AGs list and frequency\n",
    "print()\n",
    "print('AG list and frequencies')\n",
    "print(df['ag'].value_counts())\n",
    "\n",
    "#Change all object type to category\n",
    "df[df.select_dtypes(['object']).columns] = df.select_dtypes(['object']).apply(lambda x: x.astype('category'))\n",
    "\n",
    "# Display code to category mapping\n",
    "print()\n",
    "print('AG to codes mapping')\n",
    "class_to_cat_mapping = dict(enumerate(df['ag'].cat.categories))\n",
    "print(class_to_cat_mapping)\n",
    "\n",
    "#Change AG to codes\n",
    "df['ag'] = df['ag'].cat.codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'number', 'state', 'u_region', 'u_business_priority',\n",
       "       'u_classification', 'urgency', 'assigned_to', 'opened_at',\n",
       "       'u_closure_category', 'u_requester', 'u_requested_by_date',\n",
       "       'short_description', 'description', 'cmdb_ci', 'u_sla_breached',\n",
       "       'u_sla_breached_reason', 'sla_due', 'sys_updated_on', 'comments',\n",
       "       'u_bsa_comments', 'u_business_comments', 'u_developer_comments',\n",
       "       'u_tech_lead_comments', 'work_notes', 'ag_old',\n",
       "       'u_comments_and_work_notes', 'u_problem_code', 'u_problem_description',\n",
       "       'u_previous_assignment_groups', 'Requester Person ID',\n",
       "       'Requester User Id', 'Requester Full Name', 'Requester Grade',\n",
       "       'Requester Supervisor', 'Requester Cost Center Descr',\n",
       "       'Requester Location Desc', 'Assigned To Person ID',\n",
       "       'Assigned To User Id', 'Assigned To Full Name', 'Assigned To Grade',\n",
       "       'Assigned To Supervisor', 'Assigned To Cost Center Descr',\n",
       "       'Assigned To Location Desc', 'cln_desc1', 'ag', 'fulltext'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df[['u_requester', 'Requester Grade', 'Requester Supervisor', 'Requester Cost Center Descr', 'Requester Location Desc', 'ag', 'fulltext']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create important text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (4679, 47) Test shape: (774, 47)\n",
      "(4679, 49076)\n",
      "\n",
      "Shape of test tf idf:  (774, 49076)\n"
     ]
    }
   ],
   "source": [
    "#Split into test and train sets\n",
    "test = df[df['opened_at'].dt.to_period('M').astype(str).isin(test_period)].copy()\n",
    "train = df.drop(test.index, axis=0)\n",
    "print()\n",
    "print('Train shape:', train.shape, 'Test shape:', test.shape)\n",
    "\n",
    "#Remove stop words in English when creating tf idf vector and create train set\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_fit = vectorizer.fit(train['fulltext'].values)\n",
    "train_tfidf = vectorizer.transform(train['fulltext'].values)\n",
    "print(train_tfidf.shape)\n",
    "\n",
    "#Transform test set into tf idf\n",
    "docs_new = test['fulltext'].values\n",
    "X_new_tfidf = vectorizer.transform(docs_new)\n",
    "print()\n",
    "print('Shape of test tf idf: ', X_new_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run RF on tf idf and fit and find important features\n",
    "m = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "m.fit(train_tfidf, train['ag'])\n",
    "\n",
    "#Use the feature importance to find the most important words\n",
    "feature_importance = pd.DataFrame({'Feature' : vectorizer.get_feature_names(), 'Importance' : m.feature_importances_})\n",
    "feature_importance.sort_values('Importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47892"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create customer stop words\n",
    "#Consider words with importance less than 0.0001 as unimportant and remove them from tf idf\n",
    "words_to_remove = feature_importance[feature_importance['Importance'] < 0.0001]['Feature']\n",
    "\n",
    "#Add words to remove to stop words and create new tf idf\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(words_to_remove)\n",
    "len(my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (4679, 47) Test shape: (774, 47)\n",
      "(4679, 1502)\n",
      "\n",
      "Shape of test tf idf:  (774, 1502)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4679, 1502), (774, 1502))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split into test and train sets and create test and train tf idfs with reduced words\n",
    "test = df[df['opened_at'].dt.to_period('M').astype(str).isin(test_period)].copy()\n",
    "train = df.drop(test.index, axis=0)\n",
    "print()\n",
    "print('Train shape:', train.shape, 'Test shape:', test.shape)\n",
    "\n",
    "#Remove stop words in English when creating tf idf vector and create train set\n",
    "vectorizer = TfidfVectorizer(stop_words=my_stop_words)\n",
    "tfidf_fit = vectorizer.fit(train['fulltext'].values)\n",
    "train_tfidf = vectorizer.transform(train['fulltext'].values)\n",
    "print(train_tfidf.shape)\n",
    "\n",
    "#Transform test set into tf idf\n",
    "docs_new = test['fulltext'].values\n",
    "X_new_tfidf = vectorizer.transform(docs_new)\n",
    "print()\n",
    "print('Shape of test tf idf: ', X_new_tfidf.shape)\n",
    "train_tfidf.shape, X_new_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep column features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose columns needed\n",
    "to_keep = ['u_classification','Requester Grade','Requester Supervisor','Requester Cost Center Descr','Requester Location Desc','opened_at','ag']\n",
    "df_feature = df[to_keep].copy()\n",
    "\n",
    "#Change opened_at to date parts\n",
    "#add_datepart(df_feature, 'opened_at', drop=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5453, 7)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_ohe = pd.get_dummies(df_feature,columns=['u_classification','Requester Grade','Requester Supervisor','Requester Cost Center Descr','Requester Location Desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5453, 935)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (4679, 935) Test shape: (774, 935)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4679, 933), (4679,), (774, 933), (774,))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split into test and train sets\n",
    "test = df_feature_ohe[df_feature_ohe['opened_at'].dt.to_period('M').astype(str).isin(test_period)].copy()\n",
    "train = df_feature_ohe.drop(test.index, axis=0)\n",
    "print()\n",
    "print('Train shape:', train.shape, 'Test shape:', test.shape)\n",
    "\n",
    "test.drop('opened_at', axis=1, inplace=True)\n",
    "train.drop('opened_at', axis=1, inplace=True)\n",
    "\n",
    "y_test = test['ag']\n",
    "test.drop('ag', axis=1, inplace=True)\n",
    "y_train = train['ag']\n",
    "train.drop('ag', axis=1, inplace=True)\n",
    "\n",
    "train.shape, y_train.shape, test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat text and column features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4679, 2435), (774, 2435))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate train tfidf and train set to create full training set\n",
    "train_tfidf_df = pd.DataFrame(train_tfidf.todense())\n",
    "train_tfidf_df.reset_index(drop=True, inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train_full = pd.concat([train_tfidf_df, train], axis=1)\n",
    "\n",
    "#Concatenate test tfidf and test set to create full training set\n",
    "test_tfidf_df = pd.DataFrame(X_new_tfidf.todense())\n",
    "test_tfidf_df.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "test_full = pd.concat([test_tfidf_df, test], axis=1)\n",
    "\n",
    "train_full.shape, test_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run algorithms and save reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run RF on tf idf and fit\n",
    "m = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "m.fit(train_full, y_train)\n",
    "\n",
    "#Do predictions\n",
    "pred_label = m.predict(test_full)\n",
    "pred_probs = m.predict_proba(test_full)\n",
    "pred_df = pd.DataFrame(pred_probs)\n",
    "\n",
    "pred_df['first_max_label'] = pred_label\n",
    "pred_df['first_max_probs'] = pred_probs.max(axis=1)\n",
    "\n",
    "second_label = []\n",
    "for i in range(0,len(pred_probs)):\n",
    "    second_label.append(np.argsort(-pred_probs)[i][1])\n",
    "pred_df['second_max_label'] = second_label\n",
    "\n",
    "probs_list = pred_probs.copy()\n",
    "second_probs = []\n",
    "for j in range(0,len(probs_list)):\n",
    "    probs_list[j].sort()\n",
    "    second_probs.append(probs_list[j][-2])\n",
    "pred_df['second_max_probs'] = second_probs\n",
    "pred_df['actual'] = y_test.values\n",
    "pred_df['model'] = 'all_rf'\n",
    "\n",
    "#Calculate accuracy\n",
    "model_params.update({'Accuracy': accuracy_score(y_test, pred_label)})\n",
    "model_params_df = pd.DataFrame.from_dict(model_params, orient='index').T\n",
    "model_params_df['model'] = 'all_rf'\n",
    "\n",
    "#Use the feature importance to find the most important words\n",
    "feature_importance = pd.DataFrame({'Feature' : vectorizer.get_feature_names() + list(train.columns.values), 'Importance' : m.feature_importances_})\n",
    "feature_importance.sort_values('Importance', ascending=False, inplace=True)\n",
    "feature_importance['model'] = 'all_rf'\n",
    "\n",
    "#Confusion matrix analysis\n",
    "cm = confusion_matrix(y_test, pred_label)\n",
    "\n",
    "#Create cm df\n",
    "a = []\n",
    "p = []\n",
    "c = []\n",
    "t = []\n",
    "for i in range(0,cm.shape[0]):\n",
    "    for j in range(0,cm.shape[1]):\n",
    "        a.append(i)\n",
    "        p.append(j)\n",
    "        c.append(cm[i][j])\n",
    "        t.append(cm[i].sum())\n",
    "\n",
    "cm_df = pd.DataFrame({'actual': a, 'predicted':p, 'count': c, 'total_actual':t})\n",
    "cm_df['count%'] = cm_df['count'] * 100 / cm_df['total_actual']\n",
    "\n",
    "#Get the code to cat mapping as df\n",
    "class_to_cat_df = pd.DataFrame.from_dict(class_to_cat_mapping, orient='index')\n",
    "class_to_cat_df = class_to_cat_df.reset_index()\n",
    "class_to_cat_df.columns = ['code','ag']\n",
    "\n",
    "#Merge with confusion df to get names of AGs\n",
    "confusion = cm_df.merge(class_to_cat_df, left_on='actual', right_on='code', how='left')\n",
    "confusion = confusion.merge(class_to_cat_df, left_on='predicted', right_on='code', how='left')\n",
    "confusion = confusion[['ag_x', 'ag_y', 'count', 'total_actual', 'count%']].copy()\n",
    "confusion.columns = ['actual', 'predicted', 'count', 'total_actual', 'count%']\n",
    "#confusion['count%'] = confusion['count']*100/cm.sum()\n",
    "confusion['model'] = 'all_rf'\n",
    "\n",
    "with pd.ExcelWriter('all_rf.xlsx') as writer:  # doctest: +SKIP\n",
    "    pred_df.to_excel(writer, sheet_name='probability')\n",
    "    feature_importance.head(30).to_excel(writer, sheet_name='feature importance')\n",
    "    model_params_df.to_excel(writer, sheet_name='parameters')\n",
    "    confusion.to_excel(writer, sheet_name='confusion matrix')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run XGB on tf idf and fit\n",
    "m = XGBClassifier(learning_rate =0.1, n_estimators=100, max_depth=5, min_child_weight=3, gamma=0.1, subsample=0.75, colsample_bytree=0.7, objective= 'multi:softmax', nthread=4, seed=27, reg_alpha=1 ,n_jobs=-1)            \n",
    "m.fit(train_full, y_train)\n",
    "\n",
    "#Do predictions\n",
    "pred_label = m.predict(test_full)\n",
    "pred_probs = m.predict_proba(test_full)\n",
    "pred_df = pd.DataFrame(pred_probs)\n",
    "\n",
    "pred_df['first_max_label'] = pred_label\n",
    "pred_df['first_max_probs'] = pred_probs.max(axis=1)\n",
    "\n",
    "second_label = []\n",
    "for i in range(0,len(pred_probs)):\n",
    "    second_label.append(np.argsort(-pred_probs)[i][1])\n",
    "pred_df['second_max_label'] = second_label\n",
    "\n",
    "probs_list = pred_probs.copy()\n",
    "second_probs = []\n",
    "for j in range(0,len(probs_list)):\n",
    "    probs_list[j].sort()\n",
    "    second_probs.append(probs_list[j][-2])\n",
    "pred_df['second_max_probs'] = second_probs\n",
    "pred_df['actual'] = y_test.values\n",
    "pred_df['model'] = 'all_xgb'\n",
    "\n",
    "#Calculate accuracy\n",
    "model_params.update({'Accuracy': accuracy_score(y_test, pred_label)})\n",
    "model_params_df = pd.DataFrame.from_dict(model_params, orient='index').T\n",
    "model_params_df['model'] = 'all_xgb'\n",
    "\n",
    "#Use the feature importance to find the most important words\n",
    "feature_importance = pd.DataFrame({'Feature' : vectorizer.get_feature_names() + list(train.columns.values), 'Importance' : m.feature_importances_})\n",
    "feature_importance.sort_values('Importance', ascending=False, inplace=True)\n",
    "feature_importance['model'] = 'all_xgb'\n",
    "\n",
    "#Confusion matrix analysis\n",
    "cm = confusion_matrix(y_test, pred_label)\n",
    "\n",
    "#Create cm df\n",
    "a = []\n",
    "p = []\n",
    "c = []\n",
    "t = []\n",
    "for i in range(0,cm.shape[0]):\n",
    "    for j in range(0,cm.shape[1]):\n",
    "        a.append(i)\n",
    "        p.append(j)\n",
    "        c.append(cm[i][j])\n",
    "        t.append(cm[i].sum())\n",
    "\n",
    "cm_df = pd.DataFrame({'actual': a, 'predicted':p, 'count': c, 'total_actual':t})\n",
    "cm_df['count%'] = cm_df['count'] * 100 / cm_df['total_actual']\n",
    "\n",
    "#Get the code to cat mapping as df\n",
    "class_to_cat_df = pd.DataFrame.from_dict(class_to_cat_mapping, orient='index')\n",
    "class_to_cat_df = class_to_cat_df.reset_index()\n",
    "class_to_cat_df.columns = ['code','ag']\n",
    "\n",
    "#Merge with confusion df to get names of AGs\n",
    "confusion = cm_df.merge(class_to_cat_df, left_on='actual', right_on='code', how='left')\n",
    "confusion = confusion.merge(class_to_cat_df, left_on='predicted', right_on='code', how='left')\n",
    "confusion = confusion[['ag_x', 'ag_y', 'count', 'total_actual', 'count%']].copy()\n",
    "confusion.columns = ['actual', 'predicted', 'count', 'total_actual', 'count%']\n",
    "#confusion['count%'] = confusion['count']*100/cm.sum()\n",
    "confusion['model'] = 'all_xgb'\n",
    "\n",
    "with pd.ExcelWriter('all_xgb.xlsx') as writer:  # doctest: +SKIP\n",
    "    pred_df.to_excel(writer, sheet_name='probability')\n",
    "    feature_importance.head(30).to_excel(writer, sheet_name='feature importance')\n",
    "    model_params_df.to_excel(writer, sheet_name='parameters')\n",
    "    confusion.to_excel(writer, sheet_name='confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LGBMModel(objective='multiclass', n_estimators=100, n_jobs=-1, num_class=11, class_weight='balanced', importance_type='gain')\n",
    "m.fit(train_full, y_train)\n",
    "\n",
    "#Do predictions\n",
    "pred_probs = m.predict(test_full)\n",
    "pred_label = pred_probs.argmax(axis=1)\n",
    "pred_df = pd.DataFrame(pred_probs)\n",
    "\n",
    "pred_df['first_max_label'] = pred_label\n",
    "pred_df['first_max_probs'] = pred_probs.max(axis=1)\n",
    "\n",
    "second_label = []\n",
    "for i in range(0,len(pred_probs)):\n",
    "    second_label.append(np.argsort(-pred_probs)[i][1])\n",
    "pred_df['second_max_label'] = second_label\n",
    "\n",
    "probs_list = pred_probs.copy()\n",
    "second_probs = []\n",
    "for j in range(0,len(probs_list)):\n",
    "    probs_list[j].sort()\n",
    "    second_probs.append(probs_list[j][-2])\n",
    "pred_df['second_max_probs'] = second_probs\n",
    "pred_df['actual'] = y_test.values\n",
    "pred_df['model'] = 'all_lgb'\n",
    "\n",
    "#Calculate accuracy\n",
    "model_params.update({'Accuracy': accuracy_score(y_test, pred_label)})\n",
    "model_params_df = pd.DataFrame.from_dict(model_params, orient='index').T\n",
    "model_params_df['model'] = 'all_lgb'\n",
    "\n",
    "#Use the feature importance to find the most important words\n",
    "feature_importance = pd.DataFrame({'Feature' : vectorizer.get_feature_names() + list(train.columns.values), 'Importance' : m.feature_importances_})\n",
    "feature_importance.sort_values('Importance', ascending=False, inplace=True)\n",
    "feature_importance['model'] = 'all_lgb'\n",
    "\n",
    "#Confusion matrix analysis\n",
    "cm = confusion_matrix(y_test, pred_label)\n",
    "\n",
    "#Create cm df\n",
    "a = []\n",
    "p = []\n",
    "c = []\n",
    "t = []\n",
    "for i in range(0,cm.shape[0]):\n",
    "    for j in range(0,cm.shape[1]):\n",
    "        a.append(i)\n",
    "        p.append(j)\n",
    "        c.append(cm[i][j])\n",
    "        t.append(cm[i].sum())\n",
    "\n",
    "cm_df = pd.DataFrame({'actual': a, 'predicted':p, 'count': c, 'total_actual':t})\n",
    "cm_df['count%'] = cm_df['count'] * 100 / cm_df['total_actual']\n",
    "\n",
    "#Get the code to cat mapping as df\n",
    "class_to_cat_df = pd.DataFrame.from_dict(class_to_cat_mapping, orient='index')\n",
    "class_to_cat_df = class_to_cat_df.reset_index()\n",
    "class_to_cat_df.columns = ['code','ag']\n",
    "\n",
    "#Merge with confusion df to get names of AGs\n",
    "confusion = cm_df.merge(class_to_cat_df, left_on='actual', right_on='code', how='left')\n",
    "confusion = confusion.merge(class_to_cat_df, left_on='predicted', right_on='code', how='left')\n",
    "confusion = confusion[['ag_x', 'ag_y', 'count', 'total_actual', 'count%']].copy()\n",
    "confusion.columns = ['actual', 'predicted', 'count', 'total_actual', 'count%']\n",
    "#confusion['count%'] = confusion['count']*100/cm.sum()\n",
    "confusion['model'] = 'all_lgb'\n",
    "\n",
    "with pd.ExcelWriter('all_lgb.xlsx') as writer:  # doctest: +SKIP\n",
    "    pred_df.to_excel(writer, sheet_name='probability')\n",
    "    feature_importance.head(30).to_excel(writer, sheet_name='feature importance')\n",
    "    model_params_df.to_excel(writer, sheet_name='parameters')\n",
    "    confusion.to_excel(writer, sheet_name='confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LogisticRegression(multi_class='multinomial', solver='newton-cg', max_iter=1000)\n",
    "m.fit(train_full, y_train)\n",
    "\n",
    "#Do predictions\n",
    "pred_label = m.predict(test_full)\n",
    "pred_probs = m.predict_proba(test_full)\n",
    "pred_df = pd.DataFrame(pred_probs)\n",
    "\n",
    "pred_df['first_max_label'] = pred_label\n",
    "pred_df['first_max_probs'] = pred_probs.max(axis=1)\n",
    "\n",
    "second_label = []\n",
    "for i in range(0,len(pred_probs)):\n",
    "    second_label.append(np.argsort(-pred_probs)[i][1])\n",
    "pred_df['second_max_label'] = second_label\n",
    "\n",
    "probs_list = pred_probs.copy()\n",
    "second_probs = []\n",
    "for j in range(0,len(probs_list)):\n",
    "    probs_list[j].sort()\n",
    "    second_probs.append(probs_list[j][-2])\n",
    "pred_df['second_max_probs'] = second_probs\n",
    "pred_df['actual'] = y_test.values\n",
    "pred_df['model'] = 'all_log'\n",
    "\n",
    "#Calculate accuracy\n",
    "model_params.update({'Accuracy': accuracy_score(y_test, pred_label)})\n",
    "model_params_df = pd.DataFrame.from_dict(model_params, orient='index').T\n",
    "model_params_df['model'] = 'all_log'\n",
    "\n",
    "'''\n",
    "#Use the feature importance to find the most important words\n",
    "feature_importance = pd.DataFrame({'Feature' : vectorizer.get_feature_names(), 'Importance' : m.feature_importances_})\n",
    "feature_importance.sort_values('Importance', ascending=False, inplace=True)\n",
    "feature_importance['model'] = 'text_log'\n",
    "'''\n",
    "feature_importance = pd.DataFrame()\n",
    "\n",
    "#Confusion matrix analysis\n",
    "cm = confusion_matrix(y_test, pred_label)\n",
    "\n",
    "#Create cm df\n",
    "a = []\n",
    "p = []\n",
    "c = []\n",
    "t = []\n",
    "for i in range(0,cm.shape[0]):\n",
    "    for j in range(0,cm.shape[1]):\n",
    "        a.append(i)\n",
    "        p.append(j)\n",
    "        c.append(cm[i][j])\n",
    "        t.append(cm[i].sum())\n",
    "\n",
    "cm_df = pd.DataFrame({'actual': a, 'predicted':p, 'count': c, 'total_actual':t})\n",
    "cm_df['count%'] = cm_df['count'] * 100 / cm_df['total_actual']\n",
    "\n",
    "#Get the code to cat mapping as df\n",
    "class_to_cat_df = pd.DataFrame.from_dict(class_to_cat_mapping, orient='index')\n",
    "class_to_cat_df = class_to_cat_df.reset_index()\n",
    "class_to_cat_df.columns = ['code','ag']\n",
    "\n",
    "#Merge with confusion df to get names of AGs\n",
    "confusion = cm_df.merge(class_to_cat_df, left_on='actual', right_on='code', how='left')\n",
    "confusion = confusion.merge(class_to_cat_df, left_on='predicted', right_on='code', how='left')\n",
    "confusion = confusion[['ag_x', 'ag_y', 'count', 'total_actual', 'count%']].copy()\n",
    "confusion.columns = ['actual', 'predicted', 'count', 'total_actual', 'count%']\n",
    "#confusion['count%'] = confusion['count']*100/cm.sum()\n",
    "confusion['model'] = 'all_log'\n",
    "\n",
    "with pd.ExcelWriter('all_log.xlsx') as writer:  # doctest: +SKIP\n",
    "    pred_df.to_excel(writer, sheet_name='probability')\n",
    "    feature_importance.head(30).to_excel(writer, sheet_name='feature importance')\n",
    "    model_params_df.to_excel(writer, sheet_name='parameters')\n",
    "    confusion.to_excel(writer, sheet_name='confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DecisionTreeClassifier(max_depth=23)\n",
    "m.fit(train_full, y_train) \n",
    "\n",
    "#Do predictions\n",
    "pred_label = m.predict(test_full)\n",
    "pred_probs = m.predict_proba(test_full)\n",
    "pred_df = pd.DataFrame(pred_probs)\n",
    "\n",
    "pred_df['first_max_label'] = pred_label\n",
    "pred_df['first_max_probs'] = pred_probs.max(axis=1)\n",
    "\n",
    "second_label = []\n",
    "for i in range(0,len(pred_probs)):\n",
    "    second_label.append(np.argsort(-pred_probs)[i][1])\n",
    "pred_df['second_max_label'] = second_label\n",
    "\n",
    "probs_list = pred_probs.copy()\n",
    "second_probs = []\n",
    "for j in range(0,len(probs_list)):\n",
    "    probs_list[j].sort()\n",
    "    second_probs.append(probs_list[j][-2])\n",
    "pred_df['second_max_probs'] = second_probs\n",
    "pred_df['actual'] = y_test.values\n",
    "pred_df['model'] = 'all_dtc'\n",
    "\n",
    "#Calculate accuracy\n",
    "model_params.update({'Accuracy': accuracy_score(y_test, pred_label)})\n",
    "model_params_df = pd.DataFrame.from_dict(model_params, orient='index').T\n",
    "model_params_df['model'] = 'all_dtc'\n",
    "\n",
    "'''\n",
    "#Use the feature importance to find the most important words\n",
    "feature_importance = pd.DataFrame({'Feature' : vectorizer.get_feature_names(), 'Importance' : m.feature_importances_})\n",
    "feature_importance.sort_values('Importance', ascending=False, inplace=True)\n",
    "feature_importance['model'] = 'text_dtc'\n",
    "'''\n",
    "feature_importance = pd.DataFrame()\n",
    "\n",
    "#Confusion matrix analysis\n",
    "cm = confusion_matrix(y_test, pred_label)\n",
    "\n",
    "#Create cm df\n",
    "a = []\n",
    "p = []\n",
    "c = []\n",
    "t = []\n",
    "for i in range(0,cm.shape[0]):\n",
    "    for j in range(0,cm.shape[1]):\n",
    "        a.append(i)\n",
    "        p.append(j)\n",
    "        c.append(cm[i][j])\n",
    "        t.append(cm[i].sum())\n",
    "\n",
    "cm_df = pd.DataFrame({'actual': a, 'predicted':p, 'count': c, 'total_actual':t})\n",
    "cm_df['count%'] = cm_df['count'] * 100 / cm_df['total_actual']\n",
    "\n",
    "#Get the code to cat mapping as df\n",
    "class_to_cat_df = pd.DataFrame.from_dict(class_to_cat_mapping, orient='index')\n",
    "class_to_cat_df = class_to_cat_df.reset_index()\n",
    "class_to_cat_df.columns = ['code','ag']\n",
    "\n",
    "#Merge with confusion df to get names of AGs\n",
    "confusion = cm_df.merge(class_to_cat_df, left_on='actual', right_on='code', how='left')\n",
    "confusion = confusion.merge(class_to_cat_df, left_on='predicted', right_on='code', how='left')\n",
    "confusion = confusion[['ag_x', 'ag_y', 'count', 'total_actual', 'count%']].copy()\n",
    "confusion.columns = ['actual', 'predicted', 'count', 'total_actual', 'count%']\n",
    "#confusion['count%'] = confusion['count']*100/cm.sum()\n",
    "confusion['model'] = 'all_dtc'\n",
    "\n",
    "with pd.ExcelWriter('all_dtc.xlsx') as writer:  # doctest: +SKIP\n",
    "    pred_df.to_excel(writer, sheet_name='probability')\n",
    "    feature_importance.head(30).to_excel(writer, sheet_name='feature importance')\n",
    "    model_params_df.to_excel(writer, sheet_name='parameters')\n",
    "    confusion.to_excel(writer, sheet_name='confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = LinearSVC()\n",
    "m = SVC(gamma='scale', decision_function_shape='ovo', probability=True)\n",
    "m.fit(train_full, y_train) \n",
    "\n",
    "#Do predictions\n",
    "pred_label = m.predict(test_full)\n",
    "pred_probs = m.predict_proba(test_full)\n",
    "pred_df = pd.DataFrame(pred_probs)\n",
    "\n",
    "pred_df['first_max_label'] = pred_label\n",
    "pred_df['first_max_probs'] = pred_probs.max(axis=1)\n",
    "\n",
    "second_label = []\n",
    "for i in range(0,len(pred_probs)):\n",
    "    second_label.append(np.argsort(-pred_probs)[i][1])\n",
    "pred_df['second_max_label'] = second_label\n",
    "\n",
    "probs_list = pred_probs.copy()\n",
    "second_probs = []\n",
    "for j in range(0,len(probs_list)):\n",
    "    probs_list[j].sort()\n",
    "    second_probs.append(probs_list[j][-2])\n",
    "pred_df['second_max_probs'] = second_probs\n",
    "pred_df['actual'] = y_test.values\n",
    "pred_df['model'] = 'all_svm'\n",
    "\n",
    "#Calculate accuracy\n",
    "model_params.update({'Accuracy': accuracy_score(y_test, pred_label)})\n",
    "model_params_df = pd.DataFrame.from_dict(model_params, orient='index').T\n",
    "model_params_df['model'] = 'all_svm'\n",
    "\n",
    "'''\n",
    "#Use the feature importance to find the most important words\n",
    "feature_importance = pd.DataFrame({'Feature' : vectorizer.get_feature_names(), 'Importance' : m.feature_importances_})\n",
    "feature_importance.sort_values('Importance', ascending=False, inplace=True)\n",
    "feature_importance['model'] = 'text_svm'\n",
    "'''\n",
    "feature_importance = pd.DataFrame()\n",
    "\n",
    "#Confusion matrix analysis\n",
    "cm = confusion_matrix(y_test, pred_label)\n",
    "\n",
    "#Create cm df\n",
    "a = []\n",
    "p = []\n",
    "c = []\n",
    "t = []\n",
    "for i in range(0,cm.shape[0]):\n",
    "    for j in range(0,cm.shape[1]):\n",
    "        a.append(i)\n",
    "        p.append(j)\n",
    "        c.append(cm[i][j])\n",
    "        t.append(cm[i].sum())\n",
    "\n",
    "cm_df = pd.DataFrame({'actual': a, 'predicted':p, 'count': c, 'total_actual':t})\n",
    "cm_df['count%'] = cm_df['count'] * 100 / cm_df['total_actual']\n",
    "\n",
    "#Get the code to cat mapping as df\n",
    "class_to_cat_df = pd.DataFrame.from_dict(class_to_cat_mapping, orient='index')\n",
    "class_to_cat_df = class_to_cat_df.reset_index()\n",
    "class_to_cat_df.columns = ['code','ag']\n",
    "\n",
    "#Merge with confusion df to get names of AGs\n",
    "confusion = cm_df.merge(class_to_cat_df, left_on='actual', right_on='code', how='left')\n",
    "confusion = confusion.merge(class_to_cat_df, left_on='predicted', right_on='code', how='left')\n",
    "confusion = confusion[['ag_x', 'ag_y', 'count', 'total_actual', 'count%']].copy()\n",
    "confusion.columns = ['actual', 'predicted', 'count', 'total_actual', 'count%']\n",
    "#confusion['count%'] = confusion['count']*100/cm.sum()\n",
    "confusion['model'] = 'all_svm'\n",
    "\n",
    "with pd.ExcelWriter('all_svm.xlsx') as writer:  # doctest: +SKIP\n",
    "    pred_df.to_excel(writer, sheet_name='probability')\n",
    "    feature_importance.head(30).to_excel(writer, sheet_name='feature importance')\n",
    "    model_params_df.to_excel(writer, sheet_name='parameters')\n",
    "    confusion.to_excel(writer, sheet_name='confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4679, 2446), (774, 2446))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full.shape, test_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('ag')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ag'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-03cb3aaa1186>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_to_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_to_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_to_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-170-6d79a9443fd4>\u001b[0m in \u001b[0;36mdf_to_dataset\u001b[1;34m(dataframe, shuffle, batch_size)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdf_to_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m   \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ag'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m   \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mpop\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \u001b[1;36m3\u001b[0m  \u001b[0mmonkey\u001b[0m        \u001b[0mNaN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m         \"\"\"\n\u001b[1;32m--> 809\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ag'"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "train_ds = df_to_dataset(train_full, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test_full, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rf = pd.read_excel('all_rf.xlsx')\n",
    "all_xgb = pd.read_excel('all_xgb.xlsx')\n",
    "all_lgb = pd.read_excel('all_lgb.xlsx')\n",
    "all_log = pd.read_excel('all_log.xlsx')\n",
    "all_svm = pd.read_excel('all_svm.xlsx')\n",
    "all_dtc = pd.read_excel('all_dtc.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame({'all_rf':all_rf['first_max_label'], \n",
    "                         'all_xgb':all_xgb['first_max_label'], \n",
    "                         'all_lgb':all_lgb['first_max_label'],\n",
    "                         'all_dtc' : all_dtc['first_max_label'],\n",
    "                         'all_log' : all_log['first_max_label'],\n",
    "                         'all_svm' : all_svm['first_max_label']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_rf</th>\n",
       "      <th>all_xgb</th>\n",
       "      <th>all_lgb</th>\n",
       "      <th>all_dtc</th>\n",
       "      <th>all_log</th>\n",
       "      <th>all_svm</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   all_rf  all_xgb  all_lgb  all_dtc  all_log  all_svm  label\n",
       "0       7        7        7        8        7        5      7\n",
       "1       5        0        5        5        5        5      5\n",
       "2       7        7        7        7        7        7      7\n",
       "3       5        5        5        5        5        5      5\n",
       "4       5        5        5        7        8        8      5"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df['label'] = label_df.mode(axis=1)[0].astype(int)\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.789405684754522"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, label_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_consolidated = pd.read_excel('all_rf.xlsx', sheet_name='probability')\n",
    "probability_consolidated = probability_consolidated.append([pd.read_excel('all_lgb.xlsx', sheet_name='probability'),\n",
    "                                                           pd.read_excel('all_xgb.xlsx', sheet_name='probability'),\n",
    "                                                           pd.read_excel('all_log.xlsx', sheet_name='probability'),\n",
    "                                                           pd.read_excel('all_dtc.xlsx', sheet_name='probability'),\n",
    "                                                           pd.read_excel('all_svm.xlsx', sheet_name='probability')])\n",
    "probability_consolidated.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_consolidated = pd.read_excel('all_rf.xlsx', sheet_name='feature importance')\n",
    "feature_consolidated = feature_consolidated.append([pd.read_excel('all_lgb.xlsx', sheet_name='feature importance'),\n",
    "                                                    pd.read_excel('all_xgb.xlsx', sheet_name='feature importance'),\n",
    "                                                    pd.read_excel('all_log.xlsx', sheet_name='feature importance'),\n",
    "                                                    pd.read_excel('all_dtc.xlsx', sheet_name='feature importance'),\n",
    "                                                    pd.read_excel('all_svm.xlsx', sheet_name='feature importance')])\n",
    "feature_consolidated.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_consolidated = pd.read_excel('all_rf.xlsx', sheet_name='parameters')\n",
    "parameters_consolidated = parameters_consolidated.append([pd.read_excel('all_lgb.xlsx', sheet_name='parameters'),\n",
    "                                                          pd.read_excel('all_xgb.xlsx', sheet_name='parameters'),\n",
    "                                                          pd.read_excel('all_log.xlsx', sheet_name='parameters'),\n",
    "                                                          pd.read_excel('all_dtc.xlsx', sheet_name='parameters'),\n",
    "                                                          pd.read_excel('all_svm.xlsx', sheet_name='parameters')])\n",
    "parameters_consolidated.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_consolidated = pd.read_excel('all_rf.xlsx', sheet_name='confusion matrix')\n",
    "confusion_consolidated = confusion_consolidated.append([pd.read_excel('all_lgb.xlsx', sheet_name='confusion matrix'),\n",
    "                                                        pd.read_excel('all_xgb.xlsx', sheet_name='confusion matrix'),\n",
    "                                                        pd.read_excel('all_log.xlsx', sheet_name='confusion matrix'),\n",
    "                                                        pd.read_excel('all_dtc.xlsx', sheet_name='confusion matrix'),\n",
    "                                                        pd.read_excel('all_svm.xlsx', sheet_name='confusion matrix'),])\n",
    "confusion_consolidated.drop('Unnamed: 0', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('consolidated reports - text and features.xlsx') as writer:\n",
    "    probability_consolidated.to_excel(writer, sheet_name='probability')\n",
    "    feature_consolidated.to_excel(writer, sheet_name='feature importance')\n",
    "    parameters_consolidated.to_excel(writer, sheet_name='parameters')\n",
    "    confusion_consolidated.to_excel(writer, sheet_name='confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
